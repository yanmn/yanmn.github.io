<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="aaa.">
  <meta name="keywords" content="RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon_package/boxingboxing.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</h1>

          <div class="is-size-4 publication-authors">
            <p>
              <span class="author-block">CVPR 2024</span>
            </p>
          </div>
          
          <div class="is-size-5 publication-authors">
            <p>
            <span class="author-block">
              <a href="https://yanmn.github.io/">Ming Yan<sup>1,2,3</sup></a>,</span>
            <span class="author-block">
              Yan Zhang<sup>1,3</sup>,</span>
            <span class="author-block">
              Shuqiang Cai<sup>1,3</sup>,
            </span>
            <span class="author-block">
              Shuqi Fan<sup>1,3</sup>,
            </span>
            <span class="author-block">
              Xincheng Lin<sup>1,3</sup>,
            </span>
            </p>
            <span class="author-block">
              <a href="https://climbingdaily.github.io/">Yudi Dai<sup>1,3</sup></a>,</span>
            <span class="author-block">
              <a href="https://asc.xmu.edu.cn/t/shensiqi">Siqi Shen<sup>1,3*</sup></a>,</span>
            <span class="author-block">
              <a href="https://asc.xmu.edu.cn/t/wenchenglu">Chenglu Wen<sup>1,3</sup></a>,</span>
            <span class="author-block">
              <a href="https://www.xu-lan.com/">Lan Xu<sup>4</sup></a>,</span>
            <span class="author-block">
              <a href="https://yuexinma.me/">Yuexin Ma</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="http://chwang.xmu.edu.cn/">Cheng Wang<sup>1,3</sup></a>,</span>
          </div>

          <div class="is-size-6 publication-authors">
            <p>
              <span class="author-block"><sup>*</sup>Corresponding author</span>
            </p>
            <p>
             <span class="author-block"><em><sup>1</sup>Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, China</em></span>
            </p>
            <p>
              <span class="author-block"><em><sup>2</sup>National Institute for Data Science in Health and Medicine, Xiamen University, China</em></span>
            </p>
            <p>
              <span class="author-block"><em><sup>3</sup>Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, China</em></span>
            </p>
            <p>
              <span class="author-block"><em><sup>4</sup>Shanghai Engineering Research Center of Intelligent Vision and Imaging, ShanghaiTech University, China</em></span>
            </p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="./index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- ArXiv Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_RELI11D_A_Comprehensive_Multimodal_Human_Motion_Dataset_and_Method_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-philpapers"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/O32Is44hsNg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://yanmn.github.io/project_reli11d/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Data Set(Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="./index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data(coming soon)</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/intro.mp4"
                type="video/mp4">
      </video> -->
      <img src="static/images/CVPR2024_Poster_ym_website.png" width="100%">
      <br>
      <img src="static/images/Show1.png" width="100%">
      <h2 class="subtitle has-text-centered">
        RELI11D is a high-quality dataset that provides four different modalities and records movement actions(first two rows). Our dataset's annotation pipeline can provide accurate global SMPL joints, poses as well as global human motion trajectories(last row).
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We are living in a world surrounded by diverse and “smart” devices with rich modalities of sensing ability. Conveniently capturing the interactions between us humans and these objects remains far-reaching. In this paper, we present I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the human and object in a novel setting: using a minimal amount of RGB camera and object-mounted Inertial Measurement Unit (IMU). It combines general motion inference and category-aware refinement. For the former, we introduce a holistic human-object tracking method to fuse the IMU signals and the RGB stream and progressively recover the human motions and subsequently the companion object motions. For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation. It significantly refines the initial results and generates vivid body, hand, and object motions. Moreover, we contribute a large dataset with ground truth human and object motions, dense RGB inputs, and rich object-mounted IMU measurements. Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid capture setting. Our dataset and code will be released to the community.
          </p>
        </div>
      </div>
    </div>

  <div class="container is-max-desktop">
    <!-- Pipeline. -->
    <div class="hero-body">
      <h2 class="title is-3">Pipeline</h2>
      <img src="static/images/pipeline.png" width="100%">
      <h2 class="subtitle has-text-centered">
        I'm-HOI combines general motion inference and category-aware refinement.
        For the former, we introduce a holistic human-object tracking method to fuse IMU signals with RGB stream, then recover the human and companion object motions progressively.
        For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation.
        It significantly refines the initial results and generates vivid body, hand, and object motions.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <!-- Dataset. -->
    <div class="hero-body">
      <h2 class="title is-3">IMHD<sup>2</sup> Dataset</h2>
      <img src="static/images/dataset_gallery_full.png" width="100%">
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/dataset.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We exhibit sampled highlights of <i><b>I</b>nertial and <b>M</b>ulti-view <b>H</b>ighly <b>D</b>ynamic human-object interactions <b>D</b>ataset (<b>IMHD<sup>2</sup></b>)</i> on the left side, and 10 well-scanned objects on the right side. In total, our dataset records 295 sequences and captures about 892k frames of data.
      </h2>
    </div>
  </div>

  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="static\videos\nerfsynthetic.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="static\videos\shiny_blender.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="static\videos\glossy.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tanks.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div> -->
    <!--/ Matting. -->


    <!-- Matting. -->
    <!-- <div class="column">
      <h2 class="title is-3">Matting</h2>
      <div class="columns is-centered">
        <div class="column content">
          <p>
            As a byproduct of our method, we can also solve the matting problem by ignoring
            samples that fall outside of a bounding box during rendering.
          </p>
          <video id="matting-video" controls playsinline height="100%">
            <source src="./static/videos/matting.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div> -->
  <!--/ Matting. -->
<!-- </div> -->

    <!-- Comparison. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Capture Results</h2>
        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_skateboard.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_baseball.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_dumbbell.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_tennis.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_broom.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_chair.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_pan.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_kettlebell.mp4" type="video/mp4">
          </video>
        </div>

        <h2 class="subtitle has-text-centered">
          I'm-HOI performs consistently better than baselines on multiple datasets, especially on IMHD<sup>2</sup> which is characterized by fast interaction motions.
        </h2>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<!-- Related work -->
<!-- <section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Related</h2>
    <pre><code>@misc{zhao2023imhoi,
      title={I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions}, 
      author={Chengfeng Zhao and Juze Zhang and Jiashen Du and Ziwei Shan and Junye Wang and Jingyi Yu and Jingya Wang and Lan Xu},
      year={2023},
      eprint={2312.08869},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section> -->

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>@article{zhao2023imhoi,
      title={I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions},
      author={Zhao, Chengfeng and Zhang, Juze and Du, Jiashen and Shan, Ziwei and Wang, Junye and Yu, Jingyi and Wang, Jingya and Xu, Lan},
      journal={arXiv preprint arXiv:2312.08869},
      year={2023}
    }</code></pre>
  </div>
</section>

<!-- Acknowledgments -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>
      We thank Jingyan Zhang and Hongdi Yang for setting up the capture system.
      We thank Jingyan Zhang, Zining Song, Jierui Xu, Weizhi Wang, Gubin Hu, Yelin Wang, Zhiming Yu, Xuanchen Liang, af and zr for data collection.
      We thank Xiao Yu, Yuntong Liu and Xiaofan Gu for data checking and annotations.
    </p>
  </div>
</section>


<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
