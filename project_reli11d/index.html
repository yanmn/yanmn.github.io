<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="aaa.">
  <meta name="keywords" content="RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon_package/boxingboxing.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</h1>

          <div class="is-size-4 publication-authors">
            <p>
              <span class="author-block">CVPR 2024</span>
            </p>
          </div>
          
          <div class="is-size-5 publication-authors">
            <p>
            <span class="author-block">
              <a href="https://yanmn.github.io/">Ming Yan<sup>1,2,3</sup></a>,</span>
            <span class="author-block">
              Yan Zhang<sup>1,3</sup>,</span>
            <span class="author-block">
              Shuqiang Cai<sup>1,3</sup>,
            </span>
            <span class="author-block">
              Shuqi Fan<sup>1,3</sup>,
            </span>
            <span class="author-block">
              Xincheng Lin<sup>1,3</sup>,
            </span>
            </p>
            <span class="author-block">
              <a href="https://climbingdaily.github.io/">Yudi Dai<sup>1,3</sup></a>,</span>
            <span class="author-block">
              <a href="https://asc.xmu.edu.cn/t/shensiqi">Siqi Shen<sup>1,3*</sup></a>,</span>
            <span class="author-block">
              <a href="https://asc.xmu.edu.cn/t/wenchenglu">Chenglu Wen<sup>1,3</sup></a>,</span>
            <span class="author-block">
              <a href="https://www.xu-lan.com/">Lan Xu<sup>4</sup></a>,</span>
            <span class="author-block">
              <a href="https://yuexinma.me/">Yuexin Ma</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="http://chwang.xmu.edu.cn/">Cheng Wang<sup>1,3</sup></a>,</span>
          </div>

          <div class="is-size-6 publication-authors">
            <p>
              <span class="author-block"><sup>*</sup>Corresponding author</span>
            </p>
            <p>
             <span class="author-block"><em><sup>1</sup>Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, China</em></span>
            </p>
            <p>
              <span class="author-block"><em><sup>2</sup>National Institute for Data Science in Health and Medicine, Xiamen University, China</em></span>
            </p>
            <p>
              <span class="author-block"><em><sup>3</sup>Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, China</em></span>
            </p>
            <p>
              <span class="author-block"><em><sup>4</sup>Shanghai Engineering Research Center of Intelligent Vision and Imaging, ShanghaiTech University, China</em></span>
            </p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="./index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- ArXiv Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_RELI11D_A_Comprehensive_Multimodal_Human_Motion_Dataset_and_Method_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-philpapers"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/O32Is44hsNg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://yanmn.github.io/project_reli11d/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code&Dataset(Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="./index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data(coming soon)</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/intro.mp4"
                type="video/mp4">
      </video> -->
      <img src="static/images/CVPR2024_Poster_ym_website.png" width="100%">
      <br>
      <img src="static/images/Show1.png" width="100%">
      <h2 class="subtitle has-text-centered">
        RELI11D is a high-quality dataset that provides four different modalities and records movement actions(first two rows). Our dataset's annotation pipeline can provide accurate global SMPL joints, poses as well as global human motion trajectories(last row).
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present <b>RELI11D</b>, a high-quality multimodal human motion dataset involves <b>R</b>GB camera, <b>E</b>vent camera, <b>L</b>iDAR and <b>I</b>MU system. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.
          </p>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Introduction</h2>
      <h2 class="container is-max-desktop content">
        Fast and complex movements can be seen everywhere in reality. However, it is difficult to fully capture human body movements. It requires both accurately capturing complex postures and accurately positioning the human body in the scene.
      </h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p1.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Multimodal datasets can take advantage of single sensors and provide a better comprehensive understanding of human movements. RGB cameras capture appearance information. Event cameras can capture motions with high temporal resolution and dynamic range by measuring intensity change asynchronously. This fills the gaps between RGB camera frames. LiDAR is insensitive to light and can provide global geometry and trajectory information. The IMU System can achieve smooth local movements. Combining the two can make up for the shortcomings of the IMU in global coordinates.
      </h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p2.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        So, the community needs a high-quality dataset and method to fill the gap in the 3D multimodal Rapid and Complex Human Motions.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Dataset Overview</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p3.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        For holistic human motion understanding, we present RELI11D. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized four different modality and provides precise annotations.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Hardware System</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p4.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We built a portable collection system that integrates different modal hardware devices to collect in different real-world scenarios. Our MoCap system contains 17 IMUs, which record important 3D human body joint points.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Data Annotation Pipeline</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p5.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        In the Data Annotation Pipeline section. First, we input the raw data collected by different hardware devices. Highly accurate reconstructed point cloud scenes are also included. In the Data Pre-processing stage, we first separate the human point cloud from the LiDAR scene and register it with the high-precision scene. Then, synchronize and calibrate each modality. We propose a Consolidated Optimization, which includes global Pose Loss geo, human joint Loss smoo, scene awareness Loss contact, and global trajectory Loss trans. Finally, we get Global Human Poses and Trajectories that are automatically and accurately annotated.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">RELI11D GALLERY</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p6.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Baseline LEIR</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p7.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        In order to effectively integrate the data of each modality and use it for global human pose estimation, we propose a Baseline, LEIR. First, input data from different modalities. Next, we use the feature extractor corresponding to different modalities to obtain features. The modal features enter the Temporal Unified Multimodal Model, and we propose the MMCA Unit to fuse the features. Finally, in the SMPL-Based Inverse Kinematics Solver, we design a variety of Loss to constrain data of different dimensions and finally predict the global human poses. 
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">RELI11D Evaluation</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p8.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        In the qualitative experiment, we show the different stages of data set annotation, and it can be seen that our annotation has the results closest to real actions. We also manually annotate RELI11D. Table 3 shows the ablation experiments of different losses in the Consolidated Optimization Stage. The results show that both manual annotation and optimized Loss can improve the quality of the dataset.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Benchmark</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p9.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        In the Benchmarks experiment, we compare the human pose estimation method based on 2D video and the method based on global human pose estimation. It can be seen from the visualization experiments that existing methods cannot estimate fast-moving limb movements well, and almost all methods cannot estimate high-leg movements. 
        The last experiment is the evaluation of Baseline LEIR. First, we experiment with different inputs of LEIR, and the model index of three-modal input was the best. This result confirms the importance of multi-modal methods in human pose estimation. In addition, we conduct cross-dataset validation, and our method also performs better on other datasets.
      </h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Global Trajrctory Experiment</h2>
      <video id="teaser" autoplay controls muted loop preload playsinline height="100%">
        <source src="./static/videos/RELI11D_CVPR2024_480p_p10.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Furthermore, we perform visualization experiments on the prediction results of global trajectories. Combine with the analysis of the previous quantitative experimental results, it is show that the multi-modal method can be of considerable help in global human pose estimation.
      </h2>
    </div>
  </div>

</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="static\videos\nerfsynthetic.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="static\videos\shiny_blender.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="static\videos\glossy.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tanks.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


    <!-- Comparison. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Capture Results</h2>
        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_skateboard.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_baseball.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_dumbbell.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_tennis.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_broom.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_chair.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_pan.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-centered">
          <video id="replay-video" autoplay controls muted loop preload playsinline width="75%">
            <source src="./static/videos/comparison_kettlebell.mp4" type="video/mp4">
          </video>
        </div>

        <h2 class="subtitle has-text-centered">
          I'm-HOI performs consistently better than baselines on multiple datasets, especially on IMHD<sup>2</sup> which is characterized by fast interaction motions.
        </h2>
      </div>
    </div>
    / Animation. -->
  <!-- </div>
</section> -->

<!-- Related work -->
<!-- <section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Related</h2>
    <pre><code>@misc{zhao2023imhoi,
      title={I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions}, 
      author={Chengfeng Zhao and Juze Zhang and Jiashen Du and Ziwei Shan and Junye Wang and Jingyi Yu and Jingya Wang and Lan Xu},
      year={2023},
      eprint={2312.08869},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section> -->

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>
      @inproceedings{yan2024reli11d,
        title={RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method},
        author={Yan, Ming and Zhang, Yan and Cai, Shuqiang and Fan, Shuqi and Lin, Xincheng and Dai, Yudi and Shen, Siqi and Wen, Chenglu and Xu, Lan and Ma, Yuexin and others},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={2250--2262},
        year={2024}
      }
    </code></pre>
  </div>
</section>

<!-- Acknowledgments -->
<!-- <section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>
      We thank Jingyan Zhang and Hongdi Yang for setting up the capture system.
      We thank Jingyan Zhang, Zining Song, Jierui Xu, Weizhi Wang, Gubin Hu, Yelin Wang, Zhiming Yu, Xuanchen Liang, af and zr for data collection.
      We thank Xiao Yu, Yuntong Liu and Xiaofan Gu for data checking and annotations.
    </p>
  </div>
</section> -->


<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
